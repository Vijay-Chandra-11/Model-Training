{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "609e1df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract messages from WhatsApp chat\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_user_messages(txt_path: str, sender_name: str) -> list[str]:\n",
    "    pattern = re.compile(r'\\d+/\\d+/\\d+, \\d+:\\d+ (?:AM|PM) - (.*?): (.*)')\n",
    "    msgs = []\n",
    "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            m = pattern.match(line)\n",
    "            if m and m.group(1) == sender_name:\n",
    "                msgs.append(m.group(2).strip())\n",
    "    return msgs\n",
    "\n",
    "# Load messages\n",
    "txt_file = 'parsed_conversations_fixed.txt'\n",
    "sender = 'Sumith'\n",
    "messages = extract_user_messages(txt_file, sender)\n",
    "assert len(messages) > 1, 'Not enough messages to train on'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597a97f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training data written to leader_persona_train.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Convert to JSONL\n",
    "def build_jsonl(messages: list[str], out_path: str):\n",
    "    persona_desc = 'Respond like a wise, bold, visionary leader.'\n",
    "    data = []\n",
    "    for i in range(len(messages) - 1):\n",
    "        prompt = f\"{persona_desc}\\nQ: {messages[i]}\\n\"\n",
    "        response = f\"A: {messages[i + 1]}\"\n",
    "        data.append({\n",
    "            'instruction': prompt,\n",
    "            'input': '',\n",
    "            'output': response\n",
    "        })\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        for rec in data:\n",
    "            f.write(json.dumps(rec) + '\\n')\n",
    "\n",
    "jsonl_path = 'leader_persona_train.jsonl'\n",
    "build_jsonl(messages, jsonl_path)\n",
    "print(f\"\\u2705 Training data written to {jsonl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe974c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:30<00:00, 10.30s/it]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 243/243 [00:00<00:00, 3474.52 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 243/243 [00:00<00:00, 1574.63 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèã Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\Users\\vijay\\OneDrive\\Desktop\\Hack\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='183' max='183' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [183/183 4:55:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vijay\\OneDrive\\Desktop\\Hack\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training complete. Model saved to: ./leader_bot_model\n"
     ]
    }
   ],
   "source": [
    "# Step 3 & 4: Load model, apply LoRA, and train\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,\n",
    "    Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "from accelerate import dispatch_model\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 4-bit Quantization Config (to save VRAM)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load model with 4-bit quant\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Prepare for QLoRA training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA Config\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Load dataset\n",
    "jsonl_path = \"leader_persona_train.jsonl\"\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": jsonl_path})\n",
    "\n",
    "# Format for Causal LM\n",
    "def format_for_causal_lm(example):\n",
    "    return {\n",
    "        \"text\": f\"{example['instruction']}\\n{example['input']}\\n{example['output']}\"\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_for_causal_lm)\n",
    "\n",
    "# Tokenize and remove raw text\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset = dataset.remove_columns([\"instruction\", \"input\", \"output\", \"text\"])\n",
    "\n",
    "# Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./leader_bot_model\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"üèãÔ∏è Starting training...\")\n",
    "trainer.train()\n",
    "print(\"‚úÖ Training complete. Model saved to:\", training_args.output_dir)\n",
    "\n",
    "# import torch\n",
    "# from transformers import (\n",
    "#     AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig,\n",
    "#     Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "# )\n",
    "# from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig, TaskType\n",
    "# from datasets import load_dataset\n",
    "# from accelerate import init_empty_weights, infer_auto_device_map\n",
    "\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # Quantization config\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_quant_type=\"nf4\"\n",
    "# )\n",
    "\n",
    "# # Load config and infer device_map for large model support\n",
    "# config = AutoConfig.from_pretrained(model_id)\n",
    "# with init_empty_weights():\n",
    "#     model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "# device_map = infer_auto_device_map(\n",
    "#     model,\n",
    "#     max_memory={0: \"14GiB\", \"cpu\": \"32GiB\"},\n",
    "#     no_split_module_classes=[\"MistralDecoderLayer\"]\n",
    "# )\n",
    "\n",
    "# # Load model with quantization and offloading\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=device_map,\n",
    "#     torch_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "# # Prepare for QLoRA training\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# # LoRA configuration\n",
    "# lora_config = LoraConfig(\n",
    "#     r=8,\n",
    "#     lora_alpha=16,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],\n",
    "#     lora_dropout=0.1,\n",
    "#     bias=\"none\",\n",
    "#     task_type=TaskType.CAUSAL_LM\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(model, lora_config)\n",
    "\n",
    "# # Load dataset\n",
    "# jsonl_path = \"leader_persona_train.jsonl\"\n",
    "# dataset = load_dataset(\"json\", data_files={\"train\": jsonl_path})\n",
    "\n",
    "# # Preprocess dataset\n",
    "\n",
    "# def format_for_causal_lm(example):\n",
    "#     return {\n",
    "#         \"text\": f\"{example['instruction']}\\n{example['input']}\\n{example['output']}\"\n",
    "#     }\n",
    "\n",
    "# dataset = dataset.map(format_for_causal_lm)\n",
    "\n",
    "# # Tokenization\n",
    "# def tokenize(example):\n",
    "#     return tokenizer(\n",
    "#         example[\"text\"],\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=512\n",
    "#     )\n",
    "\n",
    "# dataset = dataset.map(tokenize, batched=True)\n",
    "# dataset = dataset.remove_columns([\"instruction\", \"input\", \"output\", \"text\"])\n",
    "\n",
    "# # Data collator\n",
    "# collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# # Training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./leader_bot_model\",\n",
    "#     per_device_train_batch_size=1,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     learning_rate=2e-4,\n",
    "#     num_train_epochs=3,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     save_steps=100,\n",
    "#     save_total_limit=1,\n",
    "#     fp16=True,\n",
    "#     report_to=\"none\",\n",
    "#     remove_unused_columns=False\n",
    "# )\n",
    "\n",
    "# # Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=dataset[\"train\"],\n",
    "#     data_collator=collator\n",
    "# )\n",
    "\n",
    "# # Train\n",
    "# print(\"\\U0001F3CB Starting training...\")\n",
    "# trainer.train()\n",
    "# print(\"‚úÖ Training complete. Model saved to:\", training_args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88d0a716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è You: What if I want to die?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Example Chat\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33müó£Ô∏è You: What if I want to die?\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mü§ñ Leader AI:\u001b[39m\u001b[33m'\u001b[39m, \u001b[43mchat_with_leader\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mHi\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mchat_with_leader\u001b[39m\u001b[34m(prompt, max_new_tokens)\u001b[39m\n\u001b[32m      3\u001b[39m persona_desc = \u001b[33m'\u001b[39m\u001b[33mRespond like a wise, bold, visionary leader.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m input_text = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpersona_desc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mQ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mA:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m inputs = \u001b[43mtokenizer\u001b[49m(input_text, return_tensors=\u001b[33m'\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m'\u001b[39m).to(model.device)\n\u001b[32m      6\u001b[39m outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 5: Inference\n",
    "def chat_with_leader(prompt: str, max_new_tokens: int = 100) -> str:\n",
    "    persona_desc = 'Respond like a wise, bold, visionary leader.'\n",
    "    input_text = f\"{persona_desc}\\nQ: {prompt}\\nA:\"\n",
    "    inputs = tokenizer(input_text, return_tensors='pt').to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example Chat\n",
    "print('üó£Ô∏è You: What if I want to die?')\n",
    "print('ü§ñ Leader AI:', chat_with_leader('Hi'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
